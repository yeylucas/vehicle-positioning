{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b980fb99",
   "metadata": {},
   "source": [
    "## Import Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d18ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.io \n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import time\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' \n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8606c205",
   "metadata": {},
   "source": [
    "## Dataset Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2dca00",
   "metadata": {},
   "source": [
    "### Read in the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c7763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl_origin = torch.load('Dataset/train_dl.pt', weights_only=False) \n",
    "valid_dl_origin = torch.load('Dataset/valid_dl.pt', weights_only=False)\n",
    "\n",
    "train_CSI = train_dl_origin.dataset[:][0]\n",
    "train_label = train_dl_origin.dataset[:][1][:,2].type(torch.LongTensor)\n",
    "\n",
    "valid_CSI = valid_dl_origin.dataset[:][0]\n",
    "valid_label = valid_dl_origin.dataset[:][1][:,2].type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7566af9",
   "metadata": {},
   "source": [
    "### CSI Processing: Take Modulus of complex matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f91b91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CSI_modulus = torch.abs(train_CSI)\n",
    "valid_CSI_modulus = torch.abs(valid_CSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "631db329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15000, 1, 4, 1632])\n",
      "torch.Size([5000, 1, 4, 1632])\n"
     ]
    }
   ],
   "source": [
    "print(train_CSI_modulus.shape)\n",
    "print(valid_CSI_modulus.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9fa7c",
   "metadata": {},
   "source": [
    "### CSI Processing: Normalize to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normalize",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15000, 1, 4, 1632])\n",
      "torch.Size([5000, 1, 4, 1632])\n",
      "torch.Size([15000])\n"
     ]
    }
   ],
   "source": [
    "# using training set statistics for both datasets to prevent data leakage. If instead used valid csi modulus - valid csi mod min, this would not accurately validate model that was trained and let model \"cheat\"\n",
    "train_min = train_CSI_modulus.min()\n",
    "train_max = train_CSI_modulus.max()\n",
    "\n",
    "train_CSI_norm = (train_CSI_modulus - train_min) / (train_max - train_min) #min max normalization: norm_value = (value - min) / (max - min)\n",
    "valid_CSI_norm = (valid_CSI_modulus - train_min) / (train_max - train_min)\n",
    "\n",
    "print(train_CSI_norm.shape)\n",
    "print(valid_CSI_norm.shape)\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepare_data",
   "metadata": {},
   "source": [
    "### Preparing Data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flatten",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15000, 6528])\n"
     ]
    }
   ],
   "source": [
    "# flattening data to be 2D\n",
    "train_CSI_ML = train_CSI_norm.reshape(train_CSI_norm.shape[0], -1) # calculating 15000 x (1 x 4 x 1632), to 1d vector\n",
    "valid_CSI_ML = valid_CSI_norm.reshape(valid_CSI_norm.shape[0], -1)\n",
    "\n",
    "print(train_CSI_ML.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ml_section",
   "metadata": {},
   "source": [
    "## Machine Learning: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logreg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time:  3.661696195602417\n",
      "Training Accuracy:  0.9553333333333334\n",
      "\n",
      "Training Precision:  0.9839494163424124\n",
      "Training Recall:  0.7605263157894737\n",
      "Training F1 Score:  0.857930449533503\n",
      "\n",
      "Validation Time:  0.0586850643157959\n",
      "Validation Accuracy:  0.9438\n",
      "\n",
      "Validation Precision:  0.96987087517934\n",
      "Validation Recall:  0.7222222222222222\n",
      "Validation F1 Score:  0.8279240661359462\n"
     ]
    }
   ],
   "source": [
    "# creating and initializaing logistic regression model\n",
    "\n",
    "randstate = 31\n",
    "logistic_regression = LogisticRegression(max_iter=1000, random_state=randstate)\n",
    "\n",
    "start_time = time.time() # recording start time\n",
    "\n",
    "logistic_regression.fit(train_CSI_ML, train_label) #training \n",
    "\n",
    "train_time = time.time() - start_time # training time\n",
    "\n",
    "# Evaluation and performance on training set\n",
    "\n",
    "train_pred = logistic_regression.predict(train_CSI_ML)      # model performance/outputs on its own training data after fit\n",
    "train_acc = accuracy_score(train_label, train_pred)         # training accuracy compared to labels\n",
    "train_precision = precision_score(train_label, train_pred)  # precision score\n",
    "train_recall = recall_score(train_label, train_pred)        # recall\n",
    "train_f1 = f1_score(train_label, train_pred)                # f1 score\n",
    "\n",
    "# Evaluating on validation set\n",
    "\n",
    "start_valid_time = time.time()                                  # start recording time\n",
    "\n",
    "valid_pred = logistic_regression.predict(valid_CSI_ML)          # model prediction\n",
    "\n",
    "valid_time = time.time() - start_valid_time                     # validation time \n",
    "\n",
    "valid_acc = accuracy_score(valid_label, valid_pred)             # validation accuracy\n",
    "\n",
    "valid_precision = precision_score(valid_label, valid_pred)      # precision \n",
    "valid_recall = recall_score(valid_label, valid_pred)            # recall\n",
    "valid_f1 = f1_score(valid_label, valid_pred)                    # f1 score\n",
    "\n",
    "\n",
    "\n",
    "print('Training Time: ', train_time)\n",
    "print('Training Accuracy: ', train_acc)\n",
    "print()\n",
    "print('Training Precision: ', train_precision)\n",
    "print('Training Recall: ', train_recall)\n",
    "print('Training F1 Score: ', train_f1)\n",
    "print()\n",
    "print('Validation Time: ', valid_time)\n",
    "print('Validation Accuracy: ', valid_acc)\n",
    "print()\n",
    "print('Validation Precision: ', valid_precision)\n",
    "print('Validation Recall: ', valid_recall)\n",
    "print('Validation F1 Score: ', valid_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f68f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "th3honuiqka",
   "metadata": {},
   "source": [
    "### Prepare Data for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sxxr0pefqwe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN shape: torch.Size([15000, 1, 4, 1632])\n"
     ]
    }
   ],
   "source": [
    "# Keeping 4D shape for CNN\n",
    "train_CSI_CNN = train_CSI_norm\n",
    "valid_CSI_CNN = valid_CSI_norm\n",
    "\n",
    "print('CNN shape:', train_CSI_CNN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cnn_section",
   "metadata": {},
   "source": [
    "## Neural Network: CNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cnn_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 6,737,090\n"
     ]
    }
   ],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__() # Conv2d(in_channels, out_channels, kernel_size, stride, padding) \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1) # in channel = 1 (like grayscale), out channel = 32 (32 filters); filter being applied to 4 rows 1632 cols \n",
    "        self.bn1 = nn.BatchNorm2d(32) # normalization to prevent distribution issues\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # pooling reduces dimension\n",
    "        self.fc1 = nn.Linear(64 * 1 * 408, 256) # fully connected layers\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU() # nonlinearity ReLU\n",
    "        self.dropout = nn.Dropout(0.3) # prevent overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Convolutional block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# [32, 1, 4, 1632]     \n",
    "# [32, 32, 4, 1632]    \n",
    "# [32, 32, 2, 816]     \n",
    "# [32, 64, 2, 816]     \n",
    "# [32, 64, 1, 408]     \n",
    "# [32, 26112]          \n",
    "# [32, 256]            \n",
    "# [32, 128]           \n",
    "# [32, 2]            \n",
    "\n",
    "\n",
    "\n",
    "model = CNNClassifier()\n",
    "\n",
    "# Total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total parameters: {total_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cnn_train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4362, Accuracy: 82.01%\n",
      "Epoch [2/10], Loss: 0.2500, Accuracy: 89.76%\n",
      "Epoch [3/10], Loss: 0.1369, Accuracy: 94.44%\n",
      "Epoch [4/10], Loss: 0.0932, Accuracy: 96.62%\n",
      "Epoch [5/10], Loss: 0.0705, Accuracy: 97.50%\n",
      "Epoch [6/10], Loss: 0.0743, Accuracy: 97.60%\n",
      "Epoch [7/10], Loss: 0.0502, Accuracy: 98.27%\n",
      "Epoch [8/10], Loss: 0.0358, Accuracy: 98.63%\n",
      "Epoch [9/10], Loss: 0.0447, Accuracy: 98.64%\n",
      "Epoch [10/10], Loss: 0.0526, Accuracy: 98.51%\n",
      "Training completed in 608.08 seconds\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss() #[32, 2] -> [[LoS_score, NLoS_score], ...] for 32 samples\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train and valid datasets\n",
    "train_dataset = TensorDataset(train_CSI_CNN, train_label)\n",
    "valid_dataset = TensorDataset(valid_CSI_CNN, valid_label)\n",
    "\n",
    "# Dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1) # Grab predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print(f'Training completed in {train_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cnn_eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[4064    0]\n",
      " [  16  920]]\n",
      "Accuracy: 0.9968\n",
      "Precision: 1.0000\n",
      "Recall: 0.9829\n",
      "F1 Score: 0.9914\n",
      "Testing time: 9.0245s\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "start_time = time.time()\n",
    "\n",
    "valid_preds = []\n",
    "valid_labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in valid_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        valid_preds.extend(predicted.tolist())\n",
    "        valid_labels_list.extend(labels.tolist())\n",
    "\n",
    "test_time = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "cm = confusion_matrix(valid_labels_list, valid_preds)\n",
    "accuracy = accuracy_score(valid_labels_list, valid_preds)\n",
    "precision = precision_score(valid_labels_list, valid_preds)\n",
    "recall = recall_score(valid_labels_list, valid_preds)\n",
    "f1 = f1_score(valid_labels_list, valid_preds)\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'Testing time: {test_time:.4f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd407f61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
